{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "mechanistic_interpreter.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahnavi-maddhuri/JahnaviMaddhuri-DukeXAI/blob/main/mechanistic_interpreter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jahnavi-maddhuri/JahnaviMaddhuri-DukeXAI/blob/main/mechanistic_interpreter.ipynb)\n",
        "\n",
        "# Interpreting an MLP that Attempts for String Reversal\n",
        "Multilayer Perceptrons have grown to be powerful feed forward neural networks made of fully connected layers that are stacked on top of each other. These layers begin with an input layer, end with an output layer and have at least one hidden layer in between. Often times, with these powerful models, interpretability goes out the window and the model user often has no idea what is happening behind the scenes of these predictions.\n",
        "\n",
        "In this notebook I try to demystify the black box to improve explainability and interpretability of these types of models. I use a very simple use case and model below to start by explaining a simpler, smaller example that can then be extrapolated to understand more complex models as well.\n",
        "\n",
        "In the example below, my MLP has one input, only one hidden layer and one output layer, keeping it simple. Next, my MLPs objective is to use one-hot-encoded strings and produce them in reverse order. I use synthetic data to fit this model with the simple use case of reversing strings of length three with from vocab size of three.\n",
        "\n",
        "To better understand what Multilayer Perceptrons were, how to understand the hidden layers and activation functions and also develop the scripts and fine tune them, I often used ChatGPT 5.1. Feel free to read the conversation here: https://chatgpt.com/c/69167c70-ee64-8327-9be2-f76bafb68287"
      ],
      "metadata": {
        "id": "nw5uQnGcBeak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Library Improts for NN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ho3hsFc0CLYv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generation\n",
        "First, I generate synthetic data below so the model can train and test on this dataset. For simplicity, my strings are only of length three and only have an alphabet with three characters, a, b and c. The generated X are 2500 records with each letter one-hot-encoded to form a matrix with dimensions (2500 x 3 x 3). For y, the output sequence are array representations of the index encodings. This is a matrix with dimensions (2500 x 3).\n",
        "\n",
        "For training and testing, I split this dataset into 2000 training samples and 500 testing samples."
      ],
      "metadata": {
        "id": "pIzmdBdGCN0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_string_dataset(n_train=2000, n_test=500, seq_length=3, alphabet=\"abc\"):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for string reversal.\n",
        "\n",
        "    X: one-hot encoded original strings\n",
        "       shape: (n_samples, seq_length, vocab_size), dtype=float32\n",
        "    y: reversed strings as integer indices\n",
        "       shape: (n_samples, seq_length), dtype=long\n",
        "    \"\"\"\n",
        "    n_samples = n_train + n_test\n",
        "    vocab = list(alphabet)\n",
        "    vocab_size = len(vocab)\n",
        "    char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "\n",
        "    X = torch.zeros(n_samples, seq_length, vocab_size, dtype=torch.float32)\n",
        "    y = torch.zeros(n_samples, seq_length, dtype=torch.long)\n",
        "\n",
        "    for n in range(n_samples):\n",
        "        # random string of exactly seq_length\n",
        "        s = ''.join(random.choice(vocab) for _ in range(seq_length))\n",
        "\n",
        "        # one-hot encode original string into X[n]\n",
        "        for t, ch in enumerate(s):\n",
        "            X[n, t, char_to_idx[ch]] = 1.0\n",
        "\n",
        "        # reversed string â†’ indices\n",
        "        s_rev = s[::-1]\n",
        "        for t, ch in enumerate(s_rev):\n",
        "            y[n, t] = char_to_idx[ch]\n",
        "\n",
        "    # Optional: shuffle before splitting\n",
        "    perm = torch.randperm(n_samples)\n",
        "    X = X[perm]\n",
        "    y = y[perm]\n",
        "\n",
        "    # Train/test split\n",
        "    X_train = X[:n_train]\n",
        "    y_train = y[:n_train]\n",
        "    X_test  = X[n_train:n_train + n_test]\n",
        "    y_test  = y[n_train:n_train + n_test]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, vocab, char_to_idx"
      ],
      "metadata": {
        "id": "xj3f7FGPCQkf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test, vocab, char_to_idx = generate_string_dataset()\n",
        "seq_length = X_train.size(1)\n",
        "vocab_size = X_train.size(2)\n",
        "print(f'Dimensions of X (train):{X_train.shape}')  # (2000, 3, 3)\n",
        "print(f'Dimensions of y (train):{y_train.shape}') # (2000, 3)"
      ],
      "metadata": {
        "id": "NFuEf_EuRDQl",
        "outputId": "956bebae-96df-4389-9208-288e7794ae36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of X (train):torch.Size([2000, 3, 3])\n",
            "Dimensions of y (train):torch.Size([2000, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a my String Reversal MLP\n",
        "This simple model has the architecture (1) Input Layer, (2) Hidden Layer, (3) Output Layer. My Input Layer takes in one-hot-encodings of the strings. I set my hidden layer to be of size 10. This means that there are 10 internal neurons, or features that the model uses to learn the reversal behavior. Each hidden neuron is meant to detect a slightly different pattern, and thus is tuned to a different weight through the model's training process.\n",
        "\n",
        "Below, I create the MLP, train and test the model, producing evaluation metrics at the end of the process."
      ],
      "metadata": {
        "id": "qL9twmqeCRL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StringReverseMLP(nn.Module):\n",
        "    def __init__(self, seq_length, vocab_size, hidden_dim=10):\n",
        "        super().__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Input is flattened one-hot: seq_length * vocab_size\n",
        "        input_dim = seq_length * vocab_size\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, seq_length * vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_length, vocab_size) one-hot\n",
        "        returns:\n",
        "          logits: (batch_size, seq_length, vocab_size)\n",
        "          h:      (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        x_flat = x.view(batch_size, -1)  # (batch_size, seq_length * vocab_size)\n",
        "\n",
        "        h = F.relu(self.fc1(x_flat))\n",
        "        logits_flat = self.fc2(h)  # (batch_size, seq_length * vocab_size)\n",
        "        logits = logits_flat.view(batch_size, self.seq_length, self.vocab_size)\n",
        "\n",
        "        return logits, h"
      ],
      "metadata": {
        "id": "NVlI54ibCbaf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model (one hidden layer, 10 activations)\n",
        "model = StringReverseMLP(seq_length=seq_length,\n",
        "                         vocab_size=vocab_size,\n",
        "                         hidden_dim=10)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "\n",
        "    # Forward\n",
        "    logits, h = model(X_train)           # (N, L, V)\n",
        "    N, L, V = logits.shape\n",
        "\n",
        "    # Flatten for CrossEntropyLoss\n",
        "    logits_flat = logits.view(N * L, V)  # (N*L, V)\n",
        "    targets_flat = y_train.view(N * L)   # (N*L,)\n",
        "\n",
        "    loss = criterion(logits_flat, targets_flat)\n",
        "\n",
        "    # Backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "print(\"Final training loss:\", losses[-1])\n"
      ],
      "metadata": {
        "id": "4XfhquXlSYix",
        "outputId": "05f13ee0-1e51-4277-adf6-52010b1fae9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StringReverseMLP(\n",
            "  (fc1): Linear(in_features=9, out_features=10, bias=True)\n",
            "  (fc2): Linear(in_features=10, out_features=9, bias=True)\n",
            ")\n",
            "Final training loss: 0.06964609026908875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits_val, h_val = model(X_test)\n",
        "    preds_idx = logits_val.argmax(dim=-1)  # (N, L)\n",
        "\n",
        "    # Per-position accuracy\n",
        "    per_pos_acc = (preds_idx == y_test).float().mean().item()\n",
        "\n",
        "    # Exact string accuracy (all positions correct)\n",
        "    exact_match = (preds_idx == y_test).all(dim=1).float().mean().item()\n",
        "\n",
        "print(f\"Per-position accuracy: {per_pos_acc:.2f}\")\n",
        "print(f\"Exact string accuracy: {exact_match:.2f}\")\n"
      ],
      "metadata": {
        "id": "1exP9meZS19e",
        "outputId": "a3ac0c12-5830-4f9d-950f-62269008e3a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-position accuracy: 1.00\n",
            "Exact string accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! The MLP is an excellent predictor for the reversal of size 3 strings of size 3 vocab.\n",
        "\n",
        "# Explanations\n",
        "While this accuracy is extremely high and the model appears to work well, we still don't really understand what's happening underneath the hood. Let's dive deep into a specific example from our test set to understand what our model predicted and how it got to that prediction."
      ],
      "metadata": {
        "id": "S9x6DEA1R8IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_indices(idx_tensor, vocab):\n",
        "    return ''.join(vocab[int(i)] for i in idx_tensor)\n",
        "\n",
        "sample_idx = 0\n",
        "x_sample = X_test[sample_idx:sample_idx+1]  # (1, L, V)\n",
        "y_sample = y_test[sample_idx]               # (L,)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_sample, hidden_sample = model(x_sample)\n",
        "    probs_sample = torch.softmax(logits_sample, dim=-1)  # (1, L, V)\n",
        "    pred_idx_sample = logits_sample.argmax(dim=-1).squeeze(0)  # (L,)\n",
        "\n",
        "orig_idx = X_test[sample_idx].argmax(dim=-1)  # original string indices (L,)\n",
        "orig_str = decode_indices(orig_idx, vocab)\n",
        "true_rev_str = decode_indices(y_sample, vocab)\n",
        "pred_rev_str = decode_indices(pred_idx_sample, vocab)\n",
        "\n",
        "print(\"Original string:      \", orig_str)\n",
        "print(\"True reversed string: \", true_rev_str)\n",
        "print(\"Predicted reversed:   \", pred_rev_str)\n",
        "print(\"\\nHidden layer activations:\")\n",
        "print(hidden_sample)       # (1, 10)\n",
        "print(\"\\nOutput logits:\")\n",
        "print(logits_sample)\n",
        "print(\"\\nOutput probabilities (softmax):\")\n",
        "print(probs_sample)\n"
      ],
      "metadata": {
        "id": "HJLYoa-fTJaA",
        "outputId": "6ac2196d-8485-4775-ebbc-6af4836480c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string:       bac\n",
            "True reversed string:  cab\n",
            "Predicted reversed:    cab\n",
            "\n",
            "Hidden layer activations:\n",
            "tensor([[2.3032, 0.0000, 1.0295, 0.0000, 0.0000, 3.0154, 0.0625, 0.0000, 0.2466,\n",
            "         0.0000]])\n",
            "\n",
            "Output logits:\n",
            "tensor([[[-2.0811, -2.9473,  2.0437],\n",
            "         [ 3.3705, -0.9398, -3.9562],\n",
            "         [-0.5727,  3.9886,  0.7585]]])\n",
            "\n",
            "Output probabilities (softmax):\n",
            "tensor([[[1.5803e-02, 6.6461e-03, 9.7755e-01],\n",
            "         [9.8611e-01, 1.3242e-02, 6.4856e-04],\n",
            "         [9.9509e-03, 9.5238e-01, 3.7670e-02]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvVa7vdYR2CC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}